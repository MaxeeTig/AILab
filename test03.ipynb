{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFrP2YXsSgua81y03NCkh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9078110e8abc42a59483432a2d7725dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a65bc882905479390813582dfbb208d",
              "IPY_MODEL_39c25179f4534fd9880f1ef301995c33",
              "IPY_MODEL_8d9b5230a488416ca4feb4b2cb5d9066"
            ],
            "layout": "IPY_MODEL_617264af45694fb3b345b04a6ef646d4"
          }
        },
        "8a65bc882905479390813582dfbb208d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef341eb1f0d143b78adf8226e403994d",
            "placeholder": "​",
            "style": "IPY_MODEL_5a56894fa0f84911a520451d71eb7ee0",
            "value": "llama-2-7b-chat.Q2_K.gguf: 100%"
          }
        },
        "39c25179f4534fd9880f1ef301995c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b957cd380d943509fb2aadcdb440338",
            "max": 2825940672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c0039d843cc481390b5ebeb7f9eec2c",
            "value": 2825940672
          }
        },
        "8d9b5230a488416ca4feb4b2cb5d9066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59d5a6d785304553b19e849bc5eb4123",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c32513bd0a4417a0282cb99e920620",
            "value": " 2.83G/2.83G [00:23&lt;00:00, 123MB/s]"
          }
        },
        "617264af45694fb3b345b04a6ef646d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef341eb1f0d143b78adf8226e403994d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a56894fa0f84911a520451d71eb7ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b957cd380d943509fb2aadcdb440338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0039d843cc481390b5ebeb7f9eec2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59d5a6d785304553b19e849bc5eb4123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c32513bd0a4417a0282cb99e920620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxeeTig/AILab/blob/main/test03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K-ssxnAr0wHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Installs - run once for NB\n",
        "## !pip install llama-cpp-python\n"
      ],
      "metadata": {
        "id": "-eFP8G9i0u2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "7-ePr-hA-kFg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download the model\n",
        "model_name = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "model_file = \"llama-2-7b-chat.Q2_K.gguf\"\n",
        "model_path = hf_hub_download(model_name, filename=model_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9078110e8abc42a59483432a2d7725dc",
            "8a65bc882905479390813582dfbb208d",
            "39c25179f4534fd9880f1ef301995c33",
            "8d9b5230a488416ca4feb4b2cb5d9066",
            "617264af45694fb3b345b04a6ef646d4",
            "ef341eb1f0d143b78adf8226e403994d",
            "5a56894fa0f84911a520451d71eb7ee0",
            "6b957cd380d943509fb2aadcdb440338",
            "6c0039d843cc481390b5ebeb7f9eec2c",
            "59d5a6d785304553b19e849bc5eb4123",
            "d4c32513bd0a4417a0282cb99e920620"
          ]
        },
        "id": "JLnemOLC-x1I",
        "outputId": "a181721a-f92d-4438-95af-7b7a092661ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-7b-chat.Q2_K.gguf:   0%|          | 0.00/2.83G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9078110e8abc42a59483432a2d7725dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiate model from downloaded file\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4096,  # Context length to use\n",
        "    n_threads=32,            # Number of CPU threads to use\n",
        "    n_gpu_layers=0        # Number of model layers to offload to GPU\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Rkc4SoKU-45n",
        "outputId": "f460bf66-426b-4fdf-d8a2-c5985c85d0cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Generation kwargs\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":16000,\n",
        "    \"stop\":[\"</s>\"],\n",
        "    \"echo\":False, # Echo the prompt in the output\n",
        "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
        "}"
      ],
      "metadata": {
        "id": "eN3Of-Zg--kc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Run inference loop\n",
        "while True:\n",
        "  prompt = str(input(\"Enter your question or 'bye' to finish:\"))\n",
        "  if prompt == 'bye':\n",
        "    break\n",
        "  if prompt != 'bye':\n",
        "    res = llm(prompt, **generation_kwargs)\n",
        "    print(res[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_LCnbyd9_BQA",
        "outputId": "1de97190-9078-47d8-f67f-b94a624e32a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question or 'bye' to finish:what is capibara?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    3045.55 ms\n",
            "llama_print_timings:      sample time =     530.01 ms /   695 runs   (    0.76 ms per token,  1311.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3045.44 ms /     7 tokens (  435.06 ms per token,     2.30 tokens per second)\n",
            "llama_print_timings:        eval time =  412434.97 ms /   694 runs   (  594.29 ms per token,     1.68 tokens per second)\n",
            "llama_print_timings:       total time =  417422.65 ms /   701 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Unterscheidung between the two species is not always clear-cut, and some taxonomists group them together as Capra. The capybara is a large rodent native to South America, known for its distinctive flat face and its ability to live in a variety of aquatic environments. Here are some key differences between the two species:\n",
            "\n",
            "1. Size: Capybaras are generally larger than guinea pigs. They can weigh up to 150 pounds (68 kg), while guinea pigs typically weigh between 2 and 4 pounds (0.9 to 1.8 kg).\n",
            "\n",
            "2. Body shape: Capybaras have a more rounded body shape than guinea pigs, with a broader chest and shoulders. Guinea pigs have a slender, elongated body shape.\n",
            "\n",
            "3. Snout shape: The snouts of capybaras are flatter and wider than those of guinea pigs, which are more pointed.\n",
            "\n",
            "4. Tail length: Capybaras have longer tails than guinea pigs, which can be up to 20 inches (51 cm) long. Guinea pigs have shorter tails that are typically around 3 inches (7.6 cm) long.\n",
            "\n",
            "5. Habitat: While both species can be found in a variety of habitats, capybaras are more adapted to aquatic environments and are often found in or near waterways, lakes, and swamps. Guinea pigs are more terrestrial and prefer drier areas with less vegetation.\n",
            "\n",
            "6. Diet: Capybaras are herbivores that feed on a variety of plants, including grasses, leaves, and fruits. Guinea pigs are also herbivores, but they have a more specialized diet that includes more protein-rich foods like seeds, nuts, and insects.\n",
            "\n",
            "7. Social behavior: Both species are social animals, but capybaras are known for their more gregarious nature and are often found in large groups. Guinea pigs are more solitary and prefer to be alone or in small groups.\n",
            "\n",
            "8. Reproduction: Capybaras have a longer gestation period than guinea pigs (about 120 days versus 66 days), and they typically give birth to larger litters (up to 8 young versus 3-4 young).\n",
            "\n",
            "9. Lifespan: While both species can live for around 10-15 years in captivity, capybaras tend to have a longer lifespan than guinea pigs (up to 20 years versus 10-15 years).\n",
            "\n",
            "10. Distribution: Capybaras are found in more southern regions of South America, while guinea pigs are found in a broader range of habitats across the continent, including the Andes region and parts of Central America.\n",
            "\n",
            "In summary, capybaras and guinea pigs share some similarities as rodents, but they also have several key differences in terms of their size, body shape, snout shape, tail length, habitat, diet, social behavior, reproduction, gestation period, lifespan, and distribution.\n",
            "Enter your question or 'bye' to finish:what is moon?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3045.55 ms\n",
            "llama_print_timings:      sample time =     310.53 ms /   407 runs   (    0.76 ms per token,  1310.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1250.80 ms /     2 tokens (  625.40 ms per token,     1.60 tokens per second)\n",
            "llama_print_timings:        eval time =  237409.07 ms /   406 runs   (  584.75 ms per token,     1.71 tokens per second)\n",
            "llama_print_timings:       total time =  239609.95 ms /   408 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Unterscheidung between the Moon and other celestial bodies can be difficult because of their similar appearance. However, there are some key differences that can help you tell them apart:\n",
            "\n",
            "1. Size: The Moon is much smaller than Earth, with a diameter of about 2,159 miles (3,475 kilometers). Other celestial bodies in our solar system, such as planets and dwarf planets, are much larger than the Moon.\n",
            "\n",
            "2. Orbit: The Moon orbits around Earth, while other celestial bodies orbit around the Sun. This means that the Moon is gravitationally bound to Earth, while other celestial bodies are free to move in their own orbits.\n",
            "\n",
            "3. Composition: The Moon is composed primarily of rock and metal, while other celestial bodies may have different compositions. For example, the gas giants in our solar system (Jupiter, Saturn, Uranus, and Neptune) are mostly composed of hydrogen and helium, while the ice giants (Uranus and Neptune) are composed primarily of water, ammonia, and methane ices.\n",
            "\n",
            "4. Surface features: The Moon has a heavily cratered surface, indicating that it has been hit by many meteorites over its history. Other celestial bodies may have different types of surface features, such as volcanoes, canyons, and impact craters.\n",
            "\n",
            "5. Atmosphere: The Moon does not have an atmosphere, while other celestial bodies may have atmospheres that are thin or thick. For example, Mars has a thin atmosphere made up mostly of carbon dioxide, while Venus has a thick atmosphere composed primarily of carbon dioxide and nitrogen.\n",
            "\n",
            "By paying attention to these differences, you can tell the Moon apart from other celestial bodies in our solar system.\n",
            "Enter your question or 'bye' to finish:bye\n"
          ]
        }
      ]
    }
  ]
}